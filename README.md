# Seoul-Bike-Sharing-Demand-Prediction

In this project we approached a rule-based regression predictive model for bike-sharing demand prediction. Currently, Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes. In recent days, public rental bike-sharing is becoming popular because of increased comfort and environmental sustainability. It is important to make the rental bike available and accessible to the public, as it provides many alternatives to commuters in metropolises. There are a lot of advantages in renting bikes, this renting of the bikes reduces the number of bikes running on the roads as well as it also avoids keeping the bikes unused all day long, whether it is at work or at school. Furthermore it is the healthiest way to travel and also this model supports in making our environment greener as well as reducing the pollution and also helps in the vehicle count on the road and ultimately in the reduction of the road accidents. The csv dataset which was provided to us is ‘Seoul Bike Data’. This dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, DewpointTemperature, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour, and date information. While approaching the project  we analysed and created various models. These models are being analyzed and visualization has been done. We tried to study various models to intuitively get the best performing model for our project.


An analysis with variable importance was carried to analyze the most significant variables for all the models developed with the given data sets considered. We are getting the best results from LightGBM and CatBoost. 


We started with loading the data, then we did Exploratory Data Analysis (EDA), null values treatment, encoding of categorical columns, feature selection, and then model building. In all of these models, our accuracy ranges from 56% to 94%, also there is a scope of improvement in the adjusted R2 score after hyperparameter tuning. So the accuracy of our best model is 94% which can be said to be good for such a large dataset. This performance could be due to various reasons like the proper pattern of data, large data, or because of the relevant features.
